[{"metadata":{"name":"grafana","namespace":"monitoring","selfLink":"/api/v1/namespaces/monitoring/configmaps/grafana","uid":"2f0f75b7-c96a-11e9-89b8-c400ad16bc2d","resourceVersion":"20286353","creationTimestamp":"2019-08-28T08:02:30Z","labels":{"app":"grafana","chart":"grafana-1.14.23","heritage":"Tiller","release":"grafana"}},"data":{"dashboardproviders.yaml":"apiVersion: 1\nproviders:\n- disableDeletion: false\n  editable: true\n  folder: prometheus\n  name: default\n  options:\n    path: /var/lib/grafana/dashboards\n  orgId: 1\n  type: file\n","datasources.yaml":"apiVersion: 1\ndatasources:\n- access: proxy\n  isDefault: true\n  name: Prometheus\n  type: prometheus\n  url: http://192.168.11.6:9090\n","grafana.ini":"[analytics]\ncheck_for_updates = true\n[grafana_net]\nurl = https://grafana.net\n[log]\nmode = console\n[paths]\ndata = /var/lib/grafana/data\nlogs = /var/log/grafana\nplugins = /var/lib/grafana/plugins\nprovisioning = /etc/grafana/provisioning\n"}},{"metadata":{"name":"postgres-exporter","namespace":"monitoring","selfLink":"/api/v1/namespaces/monitoring/configmaps/postgres-exporter","uid":"bfa39de5-ca06-11e9-89b8-c400ad16bc2d","resourceVersion":"18627418","creationTimestamp":"2019-08-29T02:43:14Z","labels":{"app":"postgres-exporter","chart":"postgres-exporter-0.6.6","heritage":"Tiller","release":"postgres-exporter"}},"data":{"config.yaml":"pg_replication:\n  query: \"SELECT EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp()))::INT as lag\"\n  metrics:\n    - lag:\n        usage: \"GAUGE\"\n        description: \"Replication lag behind master in seconds\"\n\npg_postmaster:\n  query: \"SELECT pg_postmaster_start_time as start_time_seconds from pg_postmaster_start_time()\"\n  metrics:\n    - start_time_seconds:\n        usage: \"GAUGE\"\n        description: \"Time at which postmaster started\"\n\npg_stat_user_tables:\n  query: \"SELECT schemaname, relname, seq_scan, seq_tup_read, idx_scan, idx_tup_fetch, n_tup_ins, n_tup_upd, n_tup_del, n_tup_hot_upd, n_live_tup, n_dead_tup, n_mod_since_analyze, last_vacuum, last_autovacuum, last_analyze, last_autoanalyze, vacuum_count, autovacuum_count, analyze_count, autoanalyze_count FROM pg_stat_user_tables\"\n  metrics:\n    - schemaname:\n        usage: \"LABEL\"\n        description: \"Name of the schema that this table is in\"\n    - relname:\n        usage: \"LABEL\"\n        description: \"Name of this table\"\n    - seq_scan:\n        usage: \"COUNTER\"\n        description: \"Number of sequential scans initiated on this table\"\n    - seq_tup_read:\n        usage: \"COUNTER\"\n        description: \"Number of live rows fetched by sequential scans\"\n    - idx_scan:\n        usage: \"COUNTER\"\n        description: \"Number of index scans initiated on this table\"\n    - idx_tup_fetch:\n        usage: \"COUNTER\"\n        description: \"Number of live rows fetched by index scans\"\n    - n_tup_ins:\n        usage: \"COUNTER\"\n        description: \"Number of rows inserted\"\n    - n_tup_upd:\n        usage: \"COUNTER\"\n        description: \"Number of rows updated\"\n    - n_tup_del:\n        usage: \"COUNTER\"\n        description: \"Number of rows deleted\"\n    - n_tup_hot_upd:\n        usage: \"COUNTER\"\n        description: \"Number of rows HOT updated (i.e., with no separate index update required)\"\n    - n_live_tup:\n        usage: \"GAUGE\"\n        description: \"Estimated number of live rows\"\n    - n_dead_tup:\n        usage: \"GAUGE\"\n        description: \"Estimated number of dead rows\"\n    - n_mod_since_analyze:\n        usage: \"GAUGE\"\n        description: \"Estimated number of rows changed since last analyze\"\n    - last_vacuum:\n        usage: \"GAUGE\"\n        description: \"Last time at which this table was manually vacuumed (not counting VACUUM FULL)\"\n    - last_autovacuum:\n        usage: \"GAUGE\"\n        description: \"Last time at which this table was vacuumed by the autovacuum daemon\"\n    - last_analyze:\n        usage: \"GAUGE\"\n        description: \"Last time at which this table was manually analyzed\"\n    - last_autoanalyze:\n        usage: \"GAUGE\"\n        description: \"Last time at which this table was analyzed by the autovacuum daemon\"\n    - vacuum_count:\n        usage: \"COUNTER\"\n        description: \"Number of times this table has been manually vacuumed (not counting VACUUM FULL)\"\n    - autovacuum_count:\n        usage: \"COUNTER\"\n        description: \"Number of times this table has been vacuumed by the autovacuum daemon\"\n    - analyze_count:\n        usage: \"COUNTER\"\n        description: \"Number of times this table has been manually analyzed\"\n    - autoanalyze_count:\n        usage: \"COUNTER\"\n        description: \"Number of times this table has been analyzed by the autovacuum daemon\"\n\npg_database:\n  query: \" SELECT pg_database.datname, pg_database_size(pg_database.datname) as size FROM pg_database\"\n  metrics:\n    - datname:\n        usage: \"LABEL\"\n        description: \"Name of the database\"\n    - size:\n        usage: \"GAUGE\"\n        description: \"Disk space used by the database\"\n"}},{"metadata":{"name":"prometheus-alertmanager","namespace":"monitoring","selfLink":"/api/v1/namespaces/monitoring/configmaps/prometheus-alertmanager","uid":"8adabc05-ddd2-11e9-b1b9-c400ad16bc2d","resourceVersion":"24430531","creationTimestamp":"2019-09-23T07:19:55Z","labels":{"app":"prometheus","chart":"prometheus-8.4.17","component":"alertmanager","heritage":"Tiller","release":"prometheus"}},"data":{"alertmanager.yml":"global:\n  resolve_timeout: 5m\n  smtp_auth_password: Password\n  smtp_auth_username: Username\n  smtp_from: User@domain.com.tw\n  smtp_require_tls: false\n  smtp_smarthost: mailapp.domain.com.tw:25\nreceivers:\n- email_configs:\n  - send_resolved: true\n    to: User1@domain.com.tw\n  - send_resolved: true\n    to: User2@domain.com.tw\n  - send_resolved: true\n    to: User3@domain.com.tw\n  name: default-receiver\nroute:\n  group_by:\n  - alertname\n  group_interval: 5m\n  group_wait: 10s\n  receiver: default-receiver\n  repeat_interval: 24h\n  routes:\n  - match:\n      severity: ^(critical|warning)$\n"}},{"metadata":{"name":"prometheus-server","namespace":"monitoring","selfLink":"/api/v1/namespaces/monitoring/configmaps/prometheus-server","uid":"8adb07d3-ddd2-11e9-b1b9-c400ad16bc2d","resourceVersion":"24430532","creationTimestamp":"2019-09-23T07:19:55Z","labels":{"app":"prometheus","chart":"prometheus-8.4.17","component":"server","heritage":"Tiller","release":"prometheus"}},"data":{"alerts":"{}\n","prometheus.yml":"global:\n  evaluation_interval: 30s\n  scrape_interval: 30s\n  scrape_timeout: 30s\n  \nrule_files:\n- /etc/config/rules.yml\n- /etc/config/alerts\nscrape_configs:\n- job_name: prometheus\n  static_configs:\n  - targets:\n    - localhost:9090\n- bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n  job_name: kubernetes-apiservers\n  kubernetes_sd_configs:\n  - role: endpoints\n  relabel_configs:\n  - action: keep\n    regex: default;kubernetes;https\n    source_labels:\n    - __meta_kubernetes_namespace\n    - __meta_kubernetes_service_name\n    - __meta_kubernetes_endpoint_port_name\n  scheme: https\n  tls_config:\n    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n    insecure_skip_verify: true\n- bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n  job_name: kubernetes-nodes\n  kubernetes_sd_configs:\n  - role: node\n  relabel_configs:\n  - action: labelmap\n    regex: __meta_kubernetes_node_label_(.+)\n  - replacement: kubernetes.default.svc:443\n    target_label: __address__\n  - regex: (.+)\n    replacement: /api/v1/nodes/${1}/proxy/metrics\n    source_labels:\n    - __meta_kubernetes_node_name\n    target_label: __metrics_path__\n  scheme: https\n  tls_config:\n    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n    insecure_skip_verify: true\n- bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n  job_name: kubernetes-nodes-cadvisor\n  kubernetes_sd_configs:\n  - role: node\n  relabel_configs:\n  - action: labelmap\n    regex: __meta_kubernetes_node_label_(.+)\n  - replacement: kubernetes.default.svc:443\n    target_label: __address__\n  - regex: (.+)\n    replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor\n    source_labels:\n    - __meta_kubernetes_node_name\n    target_label: __metrics_path__\n  scheme: https\n  tls_config:\n    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n    insecure_skip_verify: true\n- job_name: kubernetes-service-endpoints\n  kubernetes_sd_configs:\n  - role: endpoints\n  relabel_configs:\n  - action: keep\n    regex: true\n    source_labels:\n    - __meta_kubernetes_service_annotation_prometheus_io_scrape\n  - action: replace\n    regex: (https?)\n    source_labels:\n    - __meta_kubernetes_service_annotation_prometheus_io_scheme\n    target_label: __scheme__\n  - action: replace\n    regex: (.+)\n    source_labels:\n    - __meta_kubernetes_service_annotation_prometheus_io_path\n    target_label: __metrics_path__\n  - action: replace\n    regex: ([^:]+)(?::\\d+)?;(\\d+)\n    replacement: $1:$2\n    source_labels:\n    - __address__\n    - __meta_kubernetes_service_annotation_prometheus_io_port\n    target_label: __address__\n  - action: labelmap\n    regex: __meta_kubernetes_service_label_(.+)\n  - action: replace\n    source_labels:\n    - __meta_kubernetes_namespace\n    target_label: kubernetes_namespace\n  - action: replace\n    source_labels:\n    - __meta_kubernetes_service_name\n    target_label: kubernetes_name\n  - action: replace\n    source_labels:\n    - __meta_kubernetes_pod_node_name\n    target_label: kubernetes_node\n- honor_labels: true\n  job_name: prometheus-pushgateway\n  kubernetes_sd_configs:\n  - role: service\n  relabel_configs:\n  - action: keep\n    regex: pushgateway\n    source_labels:\n    - __meta_kubernetes_service_annotation_prometheus_io_probe\n- job_name: kubernetes-services\n  kubernetes_sd_configs:\n  - role: service\n  metrics_path: /probe\n  params:\n    module:\n    - http_2xx\n  relabel_configs:\n  - action: keep\n    regex: true\n    source_labels:\n    - __meta_kubernetes_service_annotation_prometheus_io_probe\n  - source_labels:\n    - __address__\n    target_label: __param_target\n  - replacement: blackbox\n    target_label: __address__\n  - source_labels:\n    - __param_target\n    target_label: instance\n  - action: labelmap\n    regex: __meta_kubernetes_service_label_(.+)\n  - source_labels:\n    - __meta_kubernetes_namespace\n    target_label: kubernetes_namespace\n  - source_labels:\n    - __meta_kubernetes_service_name\n    target_label: kubernetes_name\n- job_name: kubernetes-pods\n  kubernetes_sd_configs:\n  - role: pod\n  relabel_configs:\n  - action: keep\n    regex: true\n    source_labels:\n    - __meta_kubernetes_pod_annotation_prometheus_io_scrape\n  - action: replace\n    regex: (.+)\n    source_labels:\n    - __meta_kubernetes_pod_annotation_prometheus_io_path\n    target_label: __metrics_path__\n  - action: replace\n    regex: ([^:]+)(?::\\d+)?;(\\d+)\n    replacement: $1:$2\n    source_labels:\n    - __address__\n    - __meta_kubernetes_pod_annotation_prometheus_io_port\n    target_label: __address__\n  - action: labelmap\n    regex: __meta_kubernetes_pod_label_(.+)\n  - action: replace\n    source_labels:\n    - __meta_kubernetes_namespace\n    target_label: kubernetes_namespace\n  - action: replace\n    source_labels:\n    - __meta_kubernetes_pod_name\n    target_label: kubernetes_pod_name\n\n- job_name: 'RabbitMQ'\n  scrape_interval: 60s\n  static_configs:\n    - targets: ['192.168.11.6:9419']\n      labels:\n        instance: RabbitMQ-192.168.11.6\n- job_name: 'Postgres'\n  scrape_interval: 60s\n  static_configs:\n    - targets: ['192.168.11.6:9187']\n      labels:\n        instance: Postgres-192.168.11.6\n- job_name: 'Mongodb'\n  scrape_interval: 30s\n  static_configs:\n    - targets: ['192.168.11.6:9216']\n      labels:\n        instance: Mongodb-192.168.11.6\n\n- job_name: 'ceph'\n  honor_labels: true\n  scrape_interval: 30s\n  static_configs:\n    - targets: ['192.168.11.6:9283']\n      labels:\n        instance: Ceph-192.168.11.6\n- job_name: 'kubernetes events'\n  honor_labels: true\n  scrape_interval: 30s\n  static_configs:\n    - targets: ['192.168.11.6:9102']\n      labels:\n        instance: KubernetesEvents-192.168.11.6\n\n#   params:\n#     module: [http_2xx]\n#   static_configs:\n#     - targets:\n#       - https://example.com\n#   relabel_configs:\n#     - source_labels: [__address__]\n#       target_label: __param_target\n#     - source_labels: [__param_target]\n#       target_label: instance\n#     - target_label: __address__\n#       replacement: prometheus-blackbox-exporter:9115\n\nalerting:\n  alertmanagers:\n  - kubernetes_sd_configs:\n      - role: pod\n    tls_config:\n      ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n    relabel_configs:\n    - source_labels: [__meta_kubernetes_namespace]\n      regex: monitoring\n      action: keep\n    - source_labels: [__meta_kubernetes_pod_label_app]\n      regex: prometheus\n      action: keep\n    - source_labels: [__meta_kubernetes_pod_label_component]\n      regex: alertmanager\n      action: keep\n    - source_labels: [__meta_kubernetes_pod_container_port_number]\n      regex:\n      action: drop\n","rules.yml":"groups:\n- name: status-healthy\n  rules:\n  - alert: InstanceDown\n    annotations:\n      description: '{{$labels.instance}}: job {{$labels.job}} has been down '\n      summery: '{{$labels.instance}}: has been down'\n      value: '{{$labels.instance}}: current status: {{ $value }}'\n    expr: up == 0\n    for: 2m\n    labels:\n      severity: critical\n  - alert: MongoDBDown\n    annotations:\n      description: '{{$labels.instance}}: job {{$labels.job}} has been down '\n      summery: '{{$labels.instance}}: has been down'\n      value: '{{$labels.instance}}: current status: {{ $value }}'\n    expr: mongodb_up == 0\n    for: 2m\n    labels:\n      severity: critical\n  - alert: PostgresDown\n    annotations:\n      description: '{{$labels.instance}}: job {{$labels.job}} has been down '\n      summery: '{{$labels.instance}}: has been down'\n      value: '{{$labels.instance}}: current status: {{ $value }}'\n    expr: pg_up == 0\n    for: 2m\n    labels:\n      severity: critical\n  - alert: RabbitmqDown\n    annotations:\n      description: '{{$labels.instance}}: job {{$labels.job}} has been down '\n      summery: '{{$labels.instance}}: has been down'\n      value: '{{$labels.instance}}: current status: {{ $value }}'\n    expr: rabbitmq_up == 0\n    for: 2m\n    labels:\n      severity: critical\n- name: deployment-available\n  rules:\n  - alert: EdgesenseOtaDown\n    annotations:\n      description: '{{$labels.deployment}} has been down '\n      summery: '{{$labels.deployment}} has been down'\n      value: '{{$labels.deployment}}: current status: {{ $value }}'\n    expr: kube_deployment_status_replicas_unavailable{deployment=\"es-edgesense-ota\"}\n      == 1\n    for: 2m\n    labels:\n      severity: critical\n  - alert: EdgesensePortalDown\n    annotations:\n      description: '{{$labels.deployment}} has been down '\n      summery: '{{$labels.deployment}} has been down'\n      value: '{{$labels.deployment}}: current status: {{ $value }}'\n    expr: kube_deployment_status_replicas_unavailable{deployment=\"es-edgesense-portal\"}\n      == 1\n    for: 2m\n    labels:\n      severity: critical\n  - alert: EdgesenseWorkerDown\n    annotations:\n      description: '{{$labels.deployment}} has been down '\n      summery: '{{$labels.deployment}} has been down'\n      value: '{{$labels.deployment}}: current status: {{ $value }}'\n    expr: kube_deployment_status_replicas_unavailable{deployment=\"es-edgesense-worker\"}\n      == 1\n    for: 2m\n    labels:\n      severity: critical\n  - alert: DeviceOnProvisioningDown\n    annotations:\n      description: '{{$labels.deployment}} has been down '\n      summery: '{{$labels.deployment}} has been down'\n      value: '{{$labels.deployment}}: current status: {{ $value }}'\n    expr: kube_deployment_status_replicas_unavailable{deployment=\"es-deviceon-provisioning\"}\n      == 1\n    for: 2m\n    labels:\n      severity: critical\n  - alert: DeviceOnPortalDown\n    annotations:\n      description: '{{$labels.deployment}} has been down '\n      summery: '{{$labels.deployment}} has been down'\n      value: '{{$labels.deployment}}: current status: {{ $value }}'\n    expr: kube_deployment_status_replicas_unavailable{deployment=\"es-deviceon-portal\"}\n      == 1\n    for: 3s\n    labels:\n      severity: critical\n  - alert: DeviceOnWorkerDown\n    annotations:\n      description: '{{$labels.deployment}} has been down '\n      summery: '{{$labels.deployment}} has been down'\n      value: '{{$labels.deployment}}: current status: {{ $value }}'\n    expr: kube_deployment_status_replicas_unavailable{deployment=\"es-deviceon-worker\"}\n      == 1\n    for: 3s\n    labels:\n      severity: critical\n- name: PersistentVolume-usage\n  rules:\n  - alert: PersistentVolumeAlert\n    annotations:\n      description: 'Usage of PV: {{$labels.persistentvolumeclaim}} in {{$labels.namespace}}\n        is over 80%'\n      summery: 'Usage of PV: {{$labels.persistentvolumeclaim}} in {{$labels.namespace}}\n        is over 80%'\n      value: '{{$labels.persistentvolumeclaim}}: current status: {{ $value }}'\n    expr: kubelet_volume_stats_used_bytes/kubelet_volume_stats_capacity_bytes \u003e 0.800\n    for: 3s\n    labels:\n      severity: warning\n"}}]